{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"mount_file_id":"1xiwzNu8nsYH2cZPONQNMt-Pmd_f9ensA","authorship_tag":"ABX9TyPiTkY19ke0/Or4U1IOd2vl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!touch '/content/drive/MyDrive/Final/dataset.csv'"],"metadata":{"id":"v88gAWSR4_XC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n"],"metadata":{"id":"M81WT3rOZWt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","import IPython\n","import math\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import random\n","from datetime import datetime\n","\n","from keras import backend as keras_backend\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, SpatialDropout2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, LeakyReLU\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from keras.callbacks import ModelCheckpoint \n","from keras.regularizers import l2\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix"],"metadata":{"id":"HcMsNQBugRcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ----------------------------\n","# Prepare training data from Metadata file\n","# ----------------------------\n","path = '/content/drive/MyDrive/Final/dataset.csv'\n","import pandas as pd\n","# to get the id part of the file\n","import numpy as np\n","\n","import os\n","\n","%matplotlib inline\n","\n","dataset_dir =  '/content/drive/MyDrive/Final/'\n","\n","metadata = pd.read_csv('/content/drive/MyDrive/Final/dataset.csv')\n","metadata.head(10)\n","\n","def assignNew(label):\n","  if label == 1 :\n","    return \"snore\"\n","  elif label == 0:\n","    return \"noise\"\n","  elif label == 2:\n","    return \"cough\"\n","  else: \n","    return -1\n","\n","metadata[\"label\"] = metadata['class'].apply(assignNew)\n","metadata.head(10)\n","metadata.shape\n"],"metadata":{"id":"1dVs6L6iSr7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_classes = ['cough','noise','snore']\n","train_df_filtered = metadata[metadata[\"label\"].isin(filtered_classes)]\n","\n","print(\"Number of dataset examples: %d\"%(train_df_filtered.shape[0]))\n","print(\"Number of Classes: %d\"%(train_df_filtered.label.nunique()))\n","print(\"\\nClasses: \",train_df_filtered.label.unique())"],"metadata":{"id":"fj2OWnjLUBkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pre-processed MFCC coefficients\n","\n","X = np.load(\"/content/drive/MyDrive/Final/X-mfcc.npy\")\n","y = np.load(\"/content/drive/MyDrive/Final/y-mfcc.npy\")\n","metadata_path = '/content/drive/MyDrive/Final/dataset.csv'\n","# Metadata\n","metadata = pd.read_csv(metadata_path)"],"metadata":{"id":"WvYRsxTEE3AB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indexes = [] \n","total = len(metadata)\n","indexes = list(range(0,total))\n","#randomize indexes\n","random.shuffle(indexes)\n","\n","# Divide the indexes into Train and Test\n","test_split_pct = 20\n","split_offset = math.floor(test_split_pct * total / 100)\n","# Split the metadata\n","test_split_idx = indexes[0:split_offset]\n","train_split_idx = indexes[split_offset:total]\n","\n","# Split the features with the same indexes\n","X_test = np.take(X, test_split_idx, axis=0)\n","y_test = np.take(y, test_split_idx, axis=0)\n","X_train = np.take(X, train_split_idx, axis=0)\n","y_train = np.take(y, train_split_idx, axis=0)\n","\n","# Also split metadata\n","test_meta = metadata.iloc[test_split_idx]\n","train_meta = metadata.iloc[train_split_idx]\n","\n","# Print status\n","print(\"Test split: {} \\t\\t Train split: {}\".format(len(test_meta), len(train_meta)))\n","print(\"X test shape: {} \\t X train shape: {}\".format(X_test.shape, X_train.shape))\n","print(\"y test shape: {} \\t\\t y train shape: {}\".format(y_test.shape, y_train.shape))"],"metadata":{"id":"7RgBK21YfE_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1.2 One hot encode labels\n","le = LabelEncoder()\n","y_test_encoded = to_categorical(le.fit_transform(y_test))\n","y_train_encoded = to_categorical(le.fit_transform(y_train))"],"metadata":{"id":"l6H8FjSNGxVj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How data should be structured\n","num_rows = 40\n","num_columns = 414\n","num_channels = 1\n","\n","# Reshape to fit the network input (channel last)\n","X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n","X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n","\n","# Total number of labels to predict (equal to the network output nodes)\n","num_labels = y_train_encoded.shape[1]\n","print(num_labels)\n","#The eventual shape of the features\n","print(X_train.shape,X_test.shape) \n","# output (2400, 40, 414, 1) (600, 40, 414, 1)\n","#       (batch size, height, width,channel)\n","\n"],"metadata":{"id":"Gv04GC5rHJLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","keras_backend.clear_session()"],"metadata":{"id":"sHhtR2BtdCEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","#Define Model\n","def create_model(spatial_dropout_rate_1=0, spatial_dropout_rate_2=0, l2_rate=0):\n","  # Create a secquential object\n","  model = Sequential()\n","  #2 convolution layer with kernel size 3x3\n","\n","  model.add(Conv2D(filters=32,kernel_size=(3,3), kernel_regularizer=l2(l2_rate),\n","                   input_shape=(num_rows, num_columns, num_channels)))\n","  model.add(LeakyReLU(alpha=0.1))\n","  model.add(BatchNormalization())\n","  model.add(SpatialDropout2D(spatial_dropout_rate_1))\n","\n","  # model.add(Conv2D(filters=32, kernel_size=(3,3), kernel_regularizer=l2(l2_rate)))\n","  # model.add(LeakyReLU(alpha=0.1))\n","  # model.add(BatchNormalization())\n","\n","#max pooling\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(SpatialDropout2D(spatial_dropout_rate_1))\n","\n","##second convolution layer\n","  model.add(Conv2D(filters=64, \n","                    kernel_size=(3, 3), \n","                    kernel_regularizer=l2(l2_rate)))\n","  model.add(LeakyReLU(alpha=0.1))\n","  model.add(BatchNormalization())\n","  model.add(SpatialDropout2D(spatial_dropout_rate_2))\n","  ##third convolution layer\n","  # model.add(Conv2D(filters=64, \n","  #                    kernel_size=(3,3), \n","  #                    kernel_regularizer=l2(l2_rate)))\n","  # model.add(LeakyReLU(alpha=0.1))\n","  # model.add(BatchNormalization())\n","\n","# Reduces each h√ów feature map to a single number by taking the average of all h,w values.\n","  model.add(GlobalAveragePooling2D())\n","\n","# Softmax output\n","  model.add(Dense(num_labels, activation='softmax'))\n","    \n","  return model\n","\n","# Regularization rates\n","spatial_dropout_rate_1 = 0.07\n","spatial_dropout_rate_2 = 0.14\n","l2_rate = 0.001\n","\n","model = create_model(spatial_dropout_rate_1, spatial_dropout_rate_2, l2_rate)\n","\n","adam = Adam(lr=1e-4, beta_1=0.99, beta_2=0.999)\n","model.compile(\n","    loss='categorical_crossentropy', \n","    metrics=['accuracy'], \n","    optimizer=adam)\n","\n","# Display model architecture summary \n","model.summary()"],"metadata":{"id":"eYn_hM7DHkWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 40\n","num_batch_size = 128\n","model_file = 'CNNtraining.tf' #content/models/Hybrid-train.hdf5\n","models_path = os.path.abspath(dataset_dir)\n","model_path = os.path.join(models_path, model_file)\n","\n","\n","# Save checkpoints\n","checkpointer = ModelCheckpoint(filepath=model_path, \n","                               verbose=1, \n","                               save_best_only=True)\n","start = datetime.now()\n","history = model.fit(X_train, \n","                    y_train_encoded, \n","                    batch_size=num_batch_size, \n","                    epochs=num_epochs, \n","                    validation_split=1/12.,\n","                    callbacks=[checkpointer], \n","                    verbose=1)\n","\n","duration = datetime.now() - start\n","print(\"Training completed in time: \", duration)"],"metadata":{"id":"sCz4D4i6TPRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, X_train, y_train, X_test, y_test):\n","  train_score = model.evaluate(X_train, y_train, verbose=0)\n","  test_score = model.evaluate(X_test, y_test, verbose=0)\n","  return train_score, test_score\n","\n","def model_evaluation_report(model, X_train, y_train, X_test, y_test, calc_normal=True):\n","  dash = '-' * 38\n","\n","# Compute scores\n","  train_score, test_score = evaluate_model(model, X_train, y_train, X_test, y_test)\n","\n","# Pint Train vs Test report\n","  print('{:<10s}{:>14s}{:>14s}'.format(\"\", \"LOSS\", \"ACCURACY\"))\n","  print(dash)\n","  print('{:<10s}{:>14.4f}{:>14.4f}'.format( \"Training:\", train_score[0], 100 * train_score[1]))\n","  print('{:<10s}{:>14.4f}{:>14.4f}'.format( \"Test:\", test_score[0], 100 * test_score[1]))\n","\n","\n","# Calculate and report normalized error difference?\n","  if (calc_normal):\n","    max_err = max(train_score[0], test_score[0])\n","    error_diff = max_err - min(train_score[0], test_score[0])\n","    normal_diff = error_diff * 100 / max_err\n","    print('{:<10s}{:>13.2f}{:>1s}'.format(\"Normal diff \", normal_diff, \"\"))\n","\"\"\"\n","    Plotting\n","\"\"\"\n","\n","def plot_train_history(history, x_ticks_vertical=False):\n","  history = history.history\n","\n","# min loss / max accs\n","  min_loss = min(history['loss'])\n","  min_val_loss = min(history['val_loss'])\n","  max_accuracy = max(history['accuracy'])\n","  max_val_accuracy = max(history['val_accuracy'])\n","# x pos for loss / acc min/max\n","  min_loss_x = history['loss'].index(min_loss)\n","  min_val_loss_x = history['val_loss'].index(min_val_loss)\n","  max_accuracy_x = history['accuracy'].index(max_accuracy)\n","  max_val_accuracy_x = history['val_accuracy'].index(max_val_accuracy)\n","\n","# summarize history for loss, display min\n","  plt.figure(figsize=(16,8))\n","  plt.plot(history['loss'], color=\"#1f77b4\", alpha=0.7)\n","  plt.plot(history['val_loss'], color=\"#ff7f0e\", linestyle=\"--\")\n","  plt.plot(min_loss_x, min_loss, marker='o', markersize=3, color=\"#1f77b4\", alpha=0.7, label='Inline label')\n","  plt.plot(min_val_loss_x, min_val_loss, marker='o', markersize=3, color=\"#ff7f0e\", alpha=7, label='Inline label')\n","  plt.title('Model loss', fontsize=20)\n","  plt.ylabel('Loss', fontsize=16)\n","  plt.xlabel('Epoch', fontsize=16)\n","  plt.legend(['Train', \n","              'Test', \n","            ('%.3f' % min_loss), \n","           ('%.3f' % min_val_loss)], loc='upper right', \n","            fancybox=True, \n","            framealpha=0.9, \n","            shadow=True, borderpad=1)\n","\n","  if (x_ticks_vertical):\n","     plt.xticks(np.arange(0, len(history['loss']), 5.0), rotation='vertical')\n","  else:\n","    plt.xticks(np.arange(0, len(history['loss']), 5.0))\n","\n","  plt.show()\n","\n","    # summarize history for accuracy, display max\n","  plt.figure(figsize=(16,6))\n","  plt.plot(history['accuracy'], alpha=0.7)\n","  plt.plot(history['val_accuracy'], linestyle=\"--\")\n","  plt.plot(max_accuracy_x, max_accuracy, marker='o', markersize=3, color=\"#1f77b4\", alpha=7)\n","  plt.plot(max_val_accuracy_x, max_val_accuracy, marker='o', markersize=3, color=\"orange\", alpha=7)\n","  plt.title('Model accuracy', fontsize=20)\n","  plt.ylabel('Accuracy', fontsize=16)\n","  plt.xlabel('Epoch', fontsize=16)\n","  plt.legend(['Train', \n","             'Test', \n","           ('%.2f' % max_accuracy), \n","           ('%.2f' % max_val_accuracy)], \n","            loc='upper left', fancybox=True, framealpha=0.9, shadow=True,borderpad=1)\n","  plt.figure(num=1, figsize=(10, 6))\n","\n","  if (x_ticks_vertical):\n","      plt.xticks(np.arange(0, len(history['accuracy']), 5.0), rotation='vertical')\n","  else:\n","      plt.xticks(np.arange(0, len(history['accuracy']), 5.0))\n","\n","  plt.show()\n","# Load best saved model\n","model = load_model('/content/drive/MyDrive/Final/CNNtrain.h5')\n","model_evaluation_report(model, X_train, y_train_encoded, X_test, y_test_encoded)\n","plot_train_history(history)\n","\n","# Generate a prediction using model.predict() \n","# and calculate it's shape:\n","print(\"Generate a prediction\")\n","prediction = model.predict(X_test[:1])\n","\n","# classes_x=labels[np.argmax(prediction,axis=1)\n","# print(\"prediction shape:\", prediction.shape)\n","# print(classes_x)"],"metadata":{"id":"5-tAn8wZ5Pso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model(model_path)\n","model_evaluation_report(model, X_train, y_train_encoded, X_test, y_test_encoded)\n","plot_train_history(history)\n","\n","from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"],"metadata":{"id":"jJpU4iXZEWx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_confusion_matrix(y_true, \n","               y_pred, \n","               classes, \n","               normalize=False):\n","\n","    # Compute confusion matrix\n","    cm = metrics.confusion_matrix(y_true, y_pred)\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    return cm\n","\n","\n","# Plots a confussion matrix\n","def plot_confusion_matrix(cm,\n","                          classes, \n","                          normalized=False, \n","                          title=None, \n","                          cmap=plt.cm.Blues,\n","                          size=(10,10)):\n","    fig, ax = plt.subplots(figsize=size)\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalized else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    fig.tight_layout()\n","    plt.show()"],"metadata":{"id":"19cq1yGDYHGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = ['Noise', 'Snore', 'Cough']\n","\n","# Predict probabilities for test set\n","y_probs = model.predict(X_test, verbose=0)\n","\n","# Get predicted labels\n","yhat_probs = np.argmax(y_probs, axis=1)\n","y_trues = np.argmax(y_test_encoded, axis=1)\n","\n","# Add \"pred\" column\n","test_meta['pred'] = yhat_probs\n","\n","# Sets decimal precision (for printing output only)\n","np.set_printoptions(precision=2)\n","# Compute confusion matrix data\n","cm = confusion_matrix(y_trues, yhat_probs)\n","\n","plot_confusion_matrix(cm,\n","                      labels, \n","                      normalized=False, \n","                      title=\"Model Performance\", \n","                      cmap=plt.cm.Blues,\n","                      size=(12,12))\n"],"metadata":{"id":"oo6Q0JRHWv5_"},"execution_count":null,"outputs":[]}]}